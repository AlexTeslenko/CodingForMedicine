{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Strokes from Admission Data\n",
    "\n",
    "In this exercise we are going to try to predict strokes from admission data with various models, and learn how to deal with the issue of class imbalance.\n",
    "\n",
    "Uses the [stroke dataset from Kaggle](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset) along with the `sklearn` and `pandas` packages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Steps\n",
    "To begin with, let's setup our notebook with the necessary packages as well as grab the data from Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Establish venv/conda/package environment\n",
    "# Commands with `%` run in the command line instead of within python so we don't have to do this within a seperate terminal!\n",
    "# In this case, we are making sure our environment/colab instance has installed the latest versions of several commonly used data science packages\n",
    "%pip install pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup matplotlib to display plots correctly within pandas\n",
    "%matplotlib inline\n",
    "\n",
    "# Import packages into our runtime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download from Kaggle\n",
    "\n",
    "Those who have downloaded and setup the kaggle CLI (Command Line Interface) can run the following command to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stroke prediction dataset into the ./data folder\n",
    "!kaggle datasets download -d fedesoriano/stroke-prediction-dataset --path ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "Now we have the data downloaded, we need to load this into a dataframe so we can explore it, and perform further analysis.\n",
    "\n",
    "We can either do this by unzipping the dataset or just loading it directly with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to unzip the file - we can load the .csv file within directly into a DataFrame (commonly notated as `df`)\n",
    "df = pd.read_csv(\"./data/stroke-prediction-dataset.zip\")\n",
    "df = df.rename({\"Residence_type\": \"residence_type\"}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to explore what data we are actually dealing with and get a *feel* for it. We should check for missing data, what data is present, and relevant datatypes!\n",
    "\n",
    "There are many ways of doing this with `pandas` ([API](https://pandas.pydata.org/docs/reference/frame.html#attributes-and-underlying-data)) - the most useful and commonly used methods being `head()`, `info()`, `describe()` initially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the first 10 entries of the dataset\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe is useful for looking at continuous data (i.e. int or float datatypes) - we can see it also has a look at data which is\n",
    "# stored as booleans, as well as ID's (i.e. the id, hypertension, heart_disease, and stroke coluns)\n",
    "\n",
    "# We can see\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can exclude the boolean or ID data by selecting certain rows for example\n",
    "\n",
    "df[[\"age\", \"avg_glucose_level\", \"bmi\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and Graphing\n",
    "\n",
    "We can explore the data further by looking at the correlation of datasets - in this case using a [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), which looks for **linear** correlations between variables (non-linear correlations will not be identified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(numeric_only=True)\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").format(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "for idx, feature in enumerate(continuous_features):\n",
    "    sns.histplot(data=df, x=feature, ax=ax[idx], kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stroke\"].value_counts(normalize=True).plot(kind=\"pie\", autopct=\"%.2f\")\n",
    "plt.xlabel(\"Stroke\")\n",
    "plt.ylabel(\"Percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the vast majority of our dataset have not had a stroke - infact less than 5% have.\n",
    "\n",
    "We will explore the implications of this later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features = [\"ever_married\", \"work_type\", \"residence_type\", \"smoking_status\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "\n",
    "for idx, axis in enumerate(fig.axes):\n",
    "    sns.histplot(df[discrete_features[idx]], ax=axis)\n",
    "    axis.tick_params(labelrotation=30)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.ylabel('Average Glucose Level (mg/dl)')\n",
    "# plt.title('Comparing Average Glucose Level to Stroke Class')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"ever_married\", \"stroke\"]).size().unstack().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model\n",
    "Now we have a feel for our dataset, we can begin building a useful model to try and predict outcomes\n",
    "\n",
    "### Test-Train Split\n",
    "We need to split our model into training sets (data which the machine learning algorithm uses to learn) and testing sets (used to validate how well our model is working).\n",
    "\n",
    "### Data Pre-processing\n",
    "In order to use our data - we will need to convert how our data is stored in order for machine learning models to interpret it. For example `Male` or `Female` can't be correctly interpreted.\n",
    "\n",
    "Continuous data (i.e. BMI, Average Blood Glucose, and so on) will need to be scaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we are missing 201 BMI values: at this point we have two options, to either drop the rows which are missing data, or to _impute_ their values. In this case, we will go for the latter!\n",
    "\n",
    "There are more complex approaches to doing this - which can be done using the [`sklearn.imputer`](https://scikit-learn.org/stable/modules/impute.html) classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the average\n",
    "mean_bmi = df[\"bmi\"].mean()\n",
    "\n",
    "# Now fill missing values with this mean\n",
    "df[\"bmi\"] = df[\"bmi\"].fillna(mean_bmi)\n",
    "\n",
    "# Check that our commands have worked!\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get rid of 'ID' column, this isn't needed!\n",
    "df = df.drop(columns=[\"id\"])\n",
    "\n",
    "# We can then develop our `X` and `y` sets\n",
    "X = df.drop(columns=[\"stroke\"])\n",
    "y = df[\"stroke\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can double check we have the right shape of data - we should have 5110 rows, with 10 features in the X set - and only a column of 1 or 0 in the y column\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect! Now to split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numerical_columns = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
    "ordinal_columns = [\"work_type\", \"smoking_status\"]\n",
    "binary_columns = [\n",
    "    \"gender\",\n",
    "    \"ever_married\",\n",
    "    \"residence_type\",\n",
    "    \"heart_disease\",\n",
    "    \"hypertension\",\n",
    "]\n",
    "\n",
    "\n",
    "numerical_transformer = make_pipeline(StandardScaler())\n",
    "ordinal_transformer = make_pipeline(OneHotEncoder())\n",
    "binary_transformer = make_pipeline(OrdinalEncoder())\n",
    "\n",
    "pipeline = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", numerical_transformer, numerical_columns),\n",
    "        (\"ord\", ordinal_transformer, ordinal_columns),\n",
    "        (\"bin\", binary_transformer, binary_columns),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_prepped = pipeline.fit_transform(X_train)\n",
    "X_test_prepped = pipeline.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# TODO: Build first models\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explain problem with unbalanced datasets\n",
    "# TODO: Accuracy paradox --> look at alternative, e.g. F1 score, AUC\n",
    "# TODO: Explore imbalanced-learn package amongst others"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ucl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e6ddcdc73d423b0f01f8ea8dade5820db75de1f7bd936da71df45ce5b0f95b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
