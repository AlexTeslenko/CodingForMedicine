{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Strokes from Admission Data\n",
    "\n",
    "**In this exercise we are going to try to predict strokes from admission data using various machine learning models**\n",
    "\n",
    "This exercise uses a [stroke dataset from Kaggle](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset).\n",
    "\n",
    "Like other exercises, this is a classification problem - where we will try to determine if people have a stroke or not from some commonly used clinical metrics. We will also learn about the problem of _class imbalance_: a particularly common issue in healthcare - and learn about some approaches to over coming it. Furthermore, we will learn how to use data pipelines: an approach that helps us clean and and transform our data into a form which is more effective for machine learning algorithms to interpret and learn with.\n",
    "\n",
    "\n",
    "In this exercise, we'll learn how to:\n",
    "- **Download data** and **load it into our Jupyter Notebook**\n",
    "- Import useful libraries like **pandas**, **sci-kit learn**, and **imbalanced-learn**\n",
    "- **Clean our data**, and **modify it with sci-kit learn pipelines**\n",
    "- Explore how we can manage **class imbalance**\n",
    "- Quantify our model with **various metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Downloading and Importing Data\n",
    "To begin with, let's setup our notebook with the necessary packages as well as grab the data from Kaggle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Establish venv/conda/package environment\n",
    "# Commands with `%` run in the command line instead of within python so we don't have to do this within a seperate terminal!\n",
    "# In this case, we are making sure our environment/colab instance has installed the latest versions of several commonly used data science packages\n",
    "%pip install pandas numpy matplotlib seaborn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup matplotlib to display plots correctly within pandas\n",
    "%matplotlib inline\n",
    "\n",
    "# Import packages into our runtime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download from Kaggle\n",
    "\n",
    "Those who have downloaded and setup the kaggle CLI (Command Line Interface) can run the following command to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the stroke prediction dataset into the ./data folder\n",
    "!kaggle datasets download -d fedesoriano/stroke-prediction-dataset --path ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Exploration\n",
    "Now we have the data downloaded, we need to load this into a dataframe so we can explore it, and perform further analysis.\n",
    "\n",
    "We can either do this by unzipping the dataset or just loading it directly with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to unzip the file - we can load the .csv file within directly into a DataFrame (commonly notated as `df`)\n",
    "df = pd.read_csv(\"./data/stroke-prediction-dataset.zip\")\n",
    "df = df.rename({\"Residence_type\": \"residence_type\"}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to explore what data we are actually dealing with and get a *feel* for it. We should check for missing data, what data is present, and relevant datatypes!\n",
    "\n",
    "There are many ways of doing this with `pandas` ([API](https://pandas.pydata.org/docs/reference/frame.html#attributes-and-underlying-data)) - the most useful and commonly used methods being `head()`, `info()`, `describe()` initially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the first 10 entries of the dataset\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe is useful for looking at continuous data (i.e. int or float datatypes) - we can see it also has a look at data which is\n",
    "# stored as booleans, as well as ID's (i.e. the id, hypertension, heart_disease, and stroke coluns)\n",
    "\n",
    "# We can see\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can exclude the boolean or ID data by selecting certain rows for example\n",
    "\n",
    "df[[\"age\", \"avg_glucose_level\", \"bmi\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation and Graphing\n",
    "\n",
    "We can explore the data further by looking at the correlation of datasets - in this case using a [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), which looks for **linear** correlations between variables (non-linear correlations will not be identified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(numeric_only=True)\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").format(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "for idx, feature in enumerate(continuous_features):\n",
    "    sns.histplot(data=df, x=feature, ax=ax[idx], kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stroke\"].value_counts(normalize=True).plot(kind=\"pie\", autopct=\"%.2f\")\n",
    "plt.xlabel(\"Stroke\")\n",
    "plt.ylabel(\"Percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the vast majority of our dataset have not had a stroke - infact less than 5% have.\n",
    "\n",
    "We will explore the implications of this later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features = [\"ever_married\", \"work_type\", \"residence_type\", \"smoking_status\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "\n",
    "for idx, axis in enumerate(fig.axes):\n",
    "    sns.histplot(df[discrete_features[idx]], ax=axis)\n",
    "    axis.tick_params(labelrotation=30)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.ylabel('Average Glucose Level (mg/dl)')\n",
    "# plt.title('Comparing Average Glucose Level to Stroke Class')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"ever_married\", \"stroke\"]).size().unstack().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Preparing our Data for Model Training\n",
    "Now we have a feel for our dataset, we can begin building a useful model to try and predict outcomes\n",
    "\n",
    "### Test-Train Split\n",
    "We need to split our model into training sets (data which the machine learning algorithm uses to learn) and testing sets (used to validate how well our model is working).\n",
    "\n",
    "### Data Pre-processing\n",
    "In order to use our data - we will need to convert how our data is stored in order for machine learning models to interpret it. For example `Male` or `Female` can't be correctly interpreted.\n",
    "\n",
    "Continuous data (i.e. BMI, Average Blood Glucose, and so on) will need to be scaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we are missing 201 BMI values: at this point we have two options, to either drop the rows which are missing data, or to _impute_ their values. In this case, we will go for the latter!\n",
    "\n",
    "There are more complex approaches to doing this - which can be done using the [`sklearn.imputer`](https://scikit-learn.org/stable/modules/impute.html) classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the average\n",
    "mean_bmi = df[\"bmi\"].mean()\n",
    "\n",
    "# Now fill missing values with this mean\n",
    "df[\"bmi\"] = df[\"bmi\"].fillna(mean_bmi)\n",
    "\n",
    "# Check that our commands have worked!\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get rid of 'ID' column, this isn't needed!\n",
    "df = df.drop(columns=[\"id\"])\n",
    "\n",
    "# We can then develop our `X` and `y` sets\n",
    "X = df.drop(columns=[\"stroke\"])\n",
    "y = df[\"stroke\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can double check we have the right shape of data - we should have 5110 rows, with 10 features in the X set - and only a column of 1 or 0 in the y column\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect! Now to split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numerical_columns = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
    "ordinal_columns = [\"work_type\", \"smoking_status\"]\n",
    "binary_columns = [\"gender\", \"ever_married\", \"residence_type\"]\n",
    "\n",
    "# These are columns we don't want to modify as they're already in a good form for ML training\n",
    "pass_columns = [\"hypertension\", \"heart_disease\"]\n",
    "\n",
    "\n",
    "numerical_transformer = make_pipeline(StandardScaler())\n",
    "ordinal_transformer = make_pipeline(OneHotEncoder())\n",
    "binary_transformer = make_pipeline(OrdinalEncoder())\n",
    "\n",
    "pipeline = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", numerical_transformer, numerical_columns),\n",
    "        (\"ord\", ordinal_transformer, ordinal_columns),\n",
    "        (\"bin\", binary_transformer, binary_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",  # Include those we aren't changing in the pipeline\n",
    ")\n",
    "\n",
    "X_train_prep = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df = pd.DataFrame(X_train_prep)\n",
    "prep_df.columns = pipeline.get_feature_names_out()\n",
    "prep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to scale the test data which we haven't yet touched\n",
    "# NB: We only TRANSFORM the test date - we don't want to fit our transformers to this data\n",
    "X_test_prep = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Making our First Model\n",
    "\n",
    "Now we have imported, cleaned, and transformed our data - it is finally in a form that we can make models to help us predict future strokes.\n",
    "\n",
    "To begin with, we will use a Logistic Regression to make these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# TODO: Build first models\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train_prep, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "# we can now predict some strokes in our test dataset - which our model has not seen!\n",
    "y_pred = logistic_regression.predict(X_test_prep)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🤔 Something Fishy Afoot\n",
    "\n",
    "Hmmm... our model has a pretty a pretty good \"accuracy\" - however if we look deeper something odd is going on.\n",
    "\n",
    "Let's have a look at what are model is guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.sum()  # Add's up how many 1's there are (i.e. how many people we have predicted have had a stroke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially our model just guesses `0` / no stroke every time and get's a pretty good accuracy.\n",
    "\n",
    "For imbalanced class problems (a common and important issue in medicine!) we need to dive deeper other metrics are more useful to understand how well our model truly works. You may remember from previous exercises the F1 Score ~ which provides a metric of how well a model is at making true positives/negatives and false positives/negatives. Unfortunately, there will always be a trade off between these - and one we need to keep in mind when designing machine learning models in healthcare.\n",
    "\n",
    "The F1 score is a *harmonic mean* of the Precision and Recall:\n",
    "\n",
    "$$\\text{F1 Score} = 2\\times\\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2TP}{2TP + FP + FN}$$\n",
    "\n",
    "We have to make decisions when we tune our model if we want more *false positives* or more *false negatives* - with real world implications. If we were deploying our model, we would have to decide if we would rather have fewer false positives - and the run risk of missing potential strokes - or predict more people were having strokes when they aren't, and run the risk of over investigation and unrequired treamtment/intervention.\n",
    "\n",
    "If you would like to learn more about this it is well worth looking at the [Machine Learning University's explainer on Precision and Recall](https://mlu-explain.github.io/precision-recall/) which nicely demonstrates how the precision-recall tradeoff works visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 Score is 0 in this case, as the model doesn't predict any cases of stroke - and so the numerator is 0 (no True Positives!).\n",
    "\n",
    "Let's check more complicated models also suffer from the same issue - to check that this isn't purely as a result of [Logistic Regression](https://mlu-explain.github.io/logistic-regression/) being unable to capture complicated relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "svm = SVC()\n",
    "forest = RandomForestClassifier()\n",
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "for model in (svm, forest, gbm):\n",
    "    model.fit(X_train_prep, y_train)\n",
    "    y_pred = model.predict(X_test_prep)\n",
    "\n",
    "    print(f\"F1 Score ({type(model).__name__}): {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Tackling Unbalanced Datasets\n",
    "An unbalanced dataset is where a *class* or outcome is much less common than another: in this case the vast majority of people don't have strokes. This can lead machine learning models to tend to predict a common class (`no stroke`) over a minority/rare class (`stroke`) as it has receives many more examples of one class over another, and is unable to learn what differentiates the two.\n",
    "\n",
    "There are various ways of approaching unbalanced datasets like these, which we will explore now:\n",
    "\n",
    "### Get More Data\n",
    "In an ideal world, we could simply collect more data to represent the *minority* class, however this is not always easy or feasible. We would have to wait many months or years to collect lots of data for strokes for example!\n",
    "\n",
    "### Weighting/Model Penalization\n",
    "We can 'penalize' our model to put a greater weighting on a minority class. In essence, this forces the model to pay more attention to the minority class\n",
    "\n",
    "### Resampling\n",
    "Essentially we are manipulating our dataset so there is a ratio of classes closer to 50:50. There are two approaches within this:\n",
    " 1. **Undersampling** - where we take a sample (either randomly, or through more intelligent means) of the **majority** class, so that the ratio is limited, or\n",
    " 2. **Oversampling** - where we expand the number of **minority** class: we can do this by duplicating instances of the minority class, or we can synthesise examples of this class.\n",
    "\n",
    "This is nicely visualised below <sup>([ref](https://blog.strands.com/unbalanced-datasets))</sup>\n",
    "\n",
    "<img src=\"https://blog.strands.com/hs-fs/hubfs/Screenshot%202019-07-18%20at%2014.15.15.png?width=1200&name=Screenshot%202019-07-18%20at%2014.15.15.png\" width=700>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting More Data\n",
    "\n",
    "Sadly, we can't get more data in this case - but what we can do is make sure the *train* and *test* set have equal numbers of each class. Fortunately `sklearn` makes this easy to do when we split our model, so we don't have to worry about one size being bigger than the other!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case - we are forcing the percentages of strokes to be the same in the train and test class\n",
    "# This is done be setting the stratify command to our outcome column of strokes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# We now need to pass our data through our pipeline again!\n",
    "X_train_prep = pipeline.fit_transform(X_train)\n",
    "X_test_prep = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, this is unlikely to make much difference - as by pure chance there would likely be a similar number of strokes in the test and train set anyway.\n",
    "\n",
    "### Model Penalisation\n",
    "\n",
    "We can teach our model to pay more attention to each class by putting a 'weight' on each class. Fortunately this is easy to do in sklearn with the `class_weight` attribute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Class Weighting ===\n",
    "# We will begin with adjusting class weights as this is easy to do in the first instance\n",
    "logistic_regression = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "# Fit our model\n",
    "logistic_regression.fit(X_train_prep, y_train)\n",
    "\n",
    "# Make some predictions\n",
    "y_pred = logistic_regression.predict(X_test_prep)\n",
    "\n",
    "# See how well it performs\n",
    "print(f\"Accuracy Score (Logistic Regression): {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score (Logistic Regression): {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try more complicated models\n",
    "svm = SVC(class_weight=\"balanced\")\n",
    "forest = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "for model in (svm, forest):\n",
    "    model.fit(X_train_prep, y_train)\n",
    "    y_pred = model.predict(X_test_prep)\n",
    "\n",
    "    print(f\"F1 Score ({type(model).__name__}): {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we finally have an F1 score greater than 0m, and our Logistic regression model is performing the best at the moment. \n",
    "\n",
    "### Under and Over Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy=0.1)\n",
    "over = RandomOverSampler(sampling_strategy=0.5)\n",
    "model = LogisticRegression()\n",
    "\n",
    "X_under, y_under = under.fit_resample(X_train_prep, y_train)\n",
    "X_combined, y_combined = over.fit_resample(X_under, y_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "svm = SVC()\n",
    "forest = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "for model in (logreg, svm, forest, knn, gbm):\n",
    "    model.fit(X_combined, y_combined)\n",
    "    y_pred = model.predict(X_test_prep)\n",
    "\n",
    "    print(f\"F1 Score ({type(model).__name__}): {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Under/Over-Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore under/overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ucl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e6ddcdc73d423b0f01f8ea8dade5820db75de1f7bd936da71df45ce5b0f95b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
